{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVSln5LcEI7G"
   },
   "source": [
    "#Natural language processing with deep learninh techniques, Project - A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtajA6flEasD"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hfojryJUEoDb"
   },
   "outputs": [],
   "source": [
    "from joblib import delayed, Parallel\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import zipfile\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJLiWFQiFdJy"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0JR9lISlFcYf"
   },
   "outputs": [],
   "source": [
    "def create_df(path):\n",
    "    with zipfile.ZipFile(path, 'r') as z:\n",
    "        z.extractall()\n",
    "        print(z.namelist())\n",
    "    with zipfile.ZipFile(path, 'r') as z:\n",
    "        train_df = pd.read_csv(z.open(\"train.csv\"))\n",
    "        test_df  = pd.read_csv(z.open(\"validation.csv\"))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eu96zItWGSjx"
   },
   "outputs": [],
   "source": [
    "# labels are integers, comments are stringa\n",
    "def process_data(df):\n",
    "    df_text = df[\"text\"].astype(str).tolist()\n",
    "    df_labels = df[\"label\"].astype(int).tolist()\n",
    "    return df_text, df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1dPEkc2GaJ1",
    "outputId": "5e4784c8-a72e-4fb4-dea6-c80d481f10c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['validation.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "path = 'data.zip'\n",
    "train_df, test_df = create_df(path)\n",
    "train_text, train_labels = process_data(train_df)\n",
    "test_text, test_labels = process_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoCtCB45HnVQ"
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus, tokenizer, batch_size=1000):\n",
    "    t0 = time()\n",
    "    tokenized_docs = []\n",
    "    unique_tokens = set()\n",
    "    for doc in corpus:\n",
    "        tokenized_docs.append(tokenizer.tokenize(doc))\n",
    "        unique_tokens.update(tokenized_docs[-1])\n",
    "    return tokenized_docs, unique_tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial time: 0.4\n",
      "Serial time: 2.0\n",
      "Found 15202 unique TRAIN tokens\n",
      "Found 4796 unique TEST tokens\n",
      "984 tokens from the TEST set are missing from the TRAIN set\n"
     ]
    }
   ],
   "source": [
    "# # found this tokenizer for twitter, need to see if we have to work with specific one\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "test_tokens, unique_test_tokens = tokenize_corpus(test_text, tokenizer)\n",
    "train_tokens, unique_train_tokens = tokenize_corpus(train_text, tokenizer)\n",
    "print(f\"Found {len(unique_train_tokens)} unique TRAIN tokens\")\n",
    "print(f\"Found {len(unique_test_tokens)} unique TEST tokens\")\n",
    "print(f\"{len(unique_test_tokens-unique_train_tokens)} tokens from the TEST set are missing from the TRAIN set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Ub10e3I6x2"
   },
   "source": [
    "### Emedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SAVQDYU7I8Jh"
   },
   "outputs": [],
   "source": [
    "def token_id_mapping(tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in train_tokens:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    token_id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "\n",
    "    for word, freq in counter.items():\n",
    "        token_id[word] = len(token_id)\n",
    "    token_id_size = len(token_id)\n",
    "    id_token = {idx: word for word, idx in token_id.items()}\n",
    "\n",
    "    return token_id, token_id_size, id_token\n",
    "\n",
    "def tokens_to_ids(tokens, vocab, unk_token=\"<unk>\"):\n",
    "    unk_idx = vocab[unk_token]\n",
    "    return [vocab.get(tok, unk_idx) for tok in tokens]\n",
    "\n",
    "def ids_to_tokens(ids, token_id, id_token, pad_token=\"<pad>\"):\n",
    "    pad_idx = token_id[pad_token]\n",
    "    return [id_token[idx] for idx in ids if idx != pad_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_glove_batch(lines, start_idx, idx):\n",
    "    \"\"\"\n",
    "    Process a batch of GloVe lines and populate dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        lines: List of text lines from GloVe file\n",
    "        start_idx: Starting index for this batch\n",
    "        token2idx: Dictionary to populate with token->index mappings\n",
    "        idx2token: Dictionary to populate with index->token mappings\n",
    "        embeddings: List to append embedding vectors to\n",
    "    \n",
    "    Returns:\n",
    "        Number of successfully processed lines\n",
    "    \"\"\"\n",
    "    processed_count = 0\n",
    "    embeddings = []\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line.strip().split()\n",
    "        if not parts:\n",
    "            continue\n",
    "            \n",
    "        token = parts[0]\n",
    "        try:\n",
    "            vec = np.asarray(parts[1:], dtype=\"float32\")\n",
    "            idx = start_idx + i\n",
    "            token2idx[token] = idx\n",
    "            idx2token[idx] = token\n",
    "            embeddings.append(vec)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            # Skip malformed lines\n",
    "            continue\n",
    "    \n",
    "    return embeddings, token2idx, idx2token\n",
    "\n",
    "\n",
    "def GloVe_embedding(embedding_dim = 200, batch_size=1000, n_jobs=40):\n",
    "\n",
    "    # Glove embedding\n",
    "    #glove_path = \"glove.twitter.27B.100d.txt\" #looks nice from kaggle, I will try the origin first\n",
    "\n",
    "    #extract original GloVe\n",
    "    glove_path = f\"glove.6B.{embedding_dim}d.txt\" #download the zip file and extract it manually cause it takes forever othewise\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    \n",
    "    t0 = time()\n",
    "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"Read {len(lines)} lines from {glove_path} in {time()-t0:.1f} seconds\")\n",
    "    \n",
    "    tasks = [delayed(process_glove_batch)(lines[acc_batch:acc_batch+batch_size], acc_batch, idx) for idx, acc_batch in enumerate(range(0, len(lines), batch_size))]\n",
    "    results = Parallel(n_jobs=n_jobs)(tasks)\n",
    "    \n",
    "    embeddings = []\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "    \n",
    "    for emb, t2i, i2t in results:\n",
    "        embeddings.extend(emb)\n",
    "        token2idx.update(t2i)\n",
    "        idx2token.update(i2t)\n",
    "    \n",
    "    print(f\"Built embedding matrix in {time()-t0:.1f} seconds\")\n",
    "    return embeddings, token2idx, idx2token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Read 400000 lines from glove.6B.200d.txt in 1.5 seconds\n",
      "Built embedding matrix in 11.3 seconds\n"
     ]
    }
   ],
   "source": [
    "embeddings, token2idx, idx2token = GloVe_embedding(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991 TRAIN tokens are missing from the GloVe vocabulary\n",
      "120 TEST tokens are missing from the GloVe vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(unique_train_tokens - set(token2idx.keys()))} TRAIN tokens are missing from the GloVe vocabulary\")\n",
    "print(f\"{len(unique_test_tokens - set(token2idx.keys()))} TEST tokens are missing from the GloVe vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3sWKyYT3vY5",
    "outputId": "7e72a346-c080-4c36-d581-b8a82c1cd7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_token_to_id, train_vocab_size, train_id_to_token = token_id_mapping(train_tokens)\n",
    "test_token_to_id, test_vocab_size, test_id_to_token = token_id_mapping(test_tokens)\n",
    "Gl_train_embedding_matrix, Gl_train_embedding_dim = GloVe_embedding(train_token_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx6aWxLmjtzF"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "-G_N9ty2kHJg"
   },
   "outputs": [],
   "source": [
    "class LSTM_data_loader(Dataset):\n",
    "    def __init__(self, tweets, labels, token2idx):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.tensor(self.tweets[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return seq, label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels = zip(*batch)\n",
    "    seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ubKaWwo4kg0s"
   },
   "outputs": [],
   "source": [
    "train_ids = [tokens_to_ids(t, train_token_to_id) for t in train_tokens]\n",
    "test_ids = [tokens_to_ids(t, test_token_to_id) for t in test_tokens]\n",
    "\n",
    "train_dataset = LSTM_data_loader(train_ids, train_labels)\n",
    "test_dataset = LSTM_data_loader(test_ids, test_labels)\n",
    "\n",
    "LSTM_train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "LSTM_test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in train_ids:\n",
    "    if 0 in s:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9zg-coOiIA0"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vEJLHsoCiM0K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, token_id_size, embed_dim, hidden_dim, num_classes, embedding_matrix):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(token_id_size, embed_dim)\n",
    "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, (h, c) = self.lstm(x)\n",
    "        logits = self.fc(h[-1])\n",
    "        return logits\n",
    "\n",
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return f'Accuracy: {100 * correct / total}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyZUdEUOilEf"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OH3D3uDDinoy"
   },
   "outputs": [],
   "source": [
    "def train_LSTM(vocab_size, embed_dim, hidden_dim, embedding_matrix, num_epochs, path):\n",
    "    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, 6, embedding_matrix)\n",
    "    loss_objective = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(LSTM_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_x, batch_y in loop:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(batch_x)\n",
    "            loss = loss_objective(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        acc = compute_accuracy(model, LSTM_test_loader)\n",
    "        print(f\"epoch {epoch+1} | loss {loss.item():.4f} | {acc}\")\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GgV0BVjwrQ6_"
   },
   "outputs": [],
   "source": [
    "def double_check(mapping_func, token_to_id, id_to_token, vocab_size, embedding_matrix, path):\n",
    "\n",
    "    model =LSTMClassifier(vocab_size, 200, 256, 6, embedding_matrix)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "\n",
    "    num_examples = 20\n",
    "    examples_printed = 0\n",
    "    true, all = 0,0\n",
    "    with torch.no_grad():\n",
    "        for x, y in LSTM_test_loader:\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                # print(\"Tweet:\", x[i])  # if x[i] is token ids, you may need to decode\n",
    "                # print(\"True label:\", y[i].item())\n",
    "                # print(\"Predicted label:\", preds[i].item())\n",
    "                # print(\"---\")\n",
    "                tweet_ids = x[i].tolist()\n",
    "                tweet_words = mapping_func(tweet_ids, token_to_id, id_to_token)\n",
    "                print(\" \".join(tweet_words))\n",
    "                if y[i].item() == preds[i].item():\n",
    "                    true += 1\n",
    "                all += 1\n",
    "                examples_printed += 1\n",
    "                if examples_printed >= num_examples:\n",
    "                    break\n",
    "            if examples_printed >= num_examples:\n",
    "                break\n",
    "    print('Accracy is:', 100*true/all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJZZGlpD1-48",
    "outputId": "5fb6428d-37b5-4a73-e034-83567cf7e246"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 31.73it/s, loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 1.6886 | Accuracy: 35.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.80it/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 | loss 1.5472 | Accuracy: 35.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.33it/s, loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 | loss 1.5728 | Accuracy: 35.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 33.13it/s, loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 | loss 1.5159 | Accuracy: 28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:14<00:00, 33.81it/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 | loss 1.5520 | Accuracy: 34.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.89it/s, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 | loss 1.3663 | Accuracy: 36.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.06it/s, loss=0.893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 | loss 0.8929 | Accuracy: 59.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.65it/s, loss=0.786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 | loss 0.7862 | Accuracy: 68.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.78it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 | loss 0.1830 | Accuracy: 90.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 32.59it/s, loss=0.529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss 0.5293 | Accuracy: 91.25\n",
      "im feeling quite sad and sorry for myself but ill snap out of it soon\n",
      "i feel like i am still looking at a blank canvas blank pieces of paper\n",
      "i feel like a faithful <unk>\n",
      "i am just feeling cranky and blue\n",
      "i can have for a treat or if i am feeling festive\n",
      "i start to feel more appreciative of what god has done for me\n",
      "i am feeling more confident that we will be able to take care of this baby\n",
      "i feel incredibly lucky just to be able to talk to her\n",
      "i feel less keen about the army every day\n",
      "i feel dirty and ashamed for saying that\n",
      "i feel bitchy but not defeated yet\n",
      "i was dribbling on mums coffee table looking out of the window and feeling very happy\n",
      "i woke up often got up around am feeling <unk> radiation and groggy\n",
      "i was feeling sentimental\n",
      "i walked out of there an hour and fifteen minutes later feeling like i had been beaten with a stick and then placed on the rack and stretched\n",
      "i never stop feeling thankful as to compare with others i considered myself lucky because i did not encounter ruthless <unk> and i did not have to witness the <unk> of others\n",
      "i didn t feel abused and quite honestly it made my day a little better\n",
      "i know what it feels like he stressed glaring down at her as she <unk> more soap onto her <unk>\n",
      "i also loved that you could really feel the desperation in these <unk> and i especially liked the emotion between knight and <unk> as theyve been together in a similar fashion to batman and robin for a long time now\n",
      "i had lunch with an old friend and it was nice but in general im not feeling energetic\n",
      "Accracy is: 95.0\n"
     ]
    }
   ],
   "source": [
    "model_path = 'Gl_lstm_model.pth'\n",
    "embed_dim, hidden_dim, num_epochs = Gl_train_embedding_dim, 256, 10\n",
    "train_LSTM(train_vocab_size, embed_dim, hidden_dim, Gl_train_embedding_matrix, num_epochs, model_path)\n",
    "double_check(ids_to_tokens, test_token_to_id, test_id_to_token, train_vocab_size, Gl_train_embedding_matrix, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaBOKsoLCH4_"
   },
   "source": [
    "## To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2N76jweCKuQ"
   },
   "source": [
    "1. Implement a GRU model.\n",
    "\n",
    "2. Experiment with hyperparameters and consider plotting a graph for epochs (to potentially find the \"optimal\" point)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HtajA6flEasD",
    "PJLiWFQiFdJy"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project_a",
   "language": "python",
   "name": "project_a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
