{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Natural language processing with deep learninh techniques, Project - A"
      ],
      "metadata": {
        "id": "nVSln5LcEI7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "HtajA6flEasD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import zipfile\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "hfojryJUEoDb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "PJLiWFQiFdJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_df(path):\n",
        "  with zipfile.ZipFile(path, 'r') as z:\n",
        "      z.extractall()\n",
        "      print(z.namelist())\n",
        "\n",
        "  with zipfile.ZipFile(path, 'r') as z:\n",
        "      train_df = pd.read_csv(z.open(\"train.csv\"))\n",
        "      test_df  = pd.read_csv(z.open(\"validation.csv\"))\n",
        "  return train_df, test_df"
      ],
      "metadata": {
        "id": "0JR9lISlFcYf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels are integers, comments are stringa\n",
        "def process_data(df):\n",
        "  df_text = df[\"text\"].astype(str).tolist()\n",
        "  df_labels = df[\"label\"].astype(int).tolist()\n",
        "  return df_text, df_labels"
      ],
      "metadata": {
        "id": "eu96zItWGSjx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'data.zip'\n",
        "train_df, test_df = create_df(path)\n",
        "train_text, train_labels = process_data(train_df)\n",
        "test_text, test_labels = process_data(test_df)"
      ],
      "metadata": {
        "id": "m1dPEkc2GaJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4784c8-a72e-4fb4-dea6-c80d481f10c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['validation.csv', 'train.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize"
      ],
      "metadata": {
        "id": "GoCtCB45HnVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# found this tokenizer for twitter, need to see if we have to work with specific one\n",
        "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "train_tokens = [tokenizer.tokenize(t) for t in train_text]\n",
        "test_tokens = [tokenizer.tokenize(t) for t in test_text]"
      ],
      "metadata": {
        "id": "xM1gK4-sHpUr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emedding"
      ],
      "metadata": {
        "id": "R5Ub10e3I6x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_id_mapping(tokens):\n",
        "  counter = Counter()\n",
        "  for tokens in train_tokens:\n",
        "      counter.update(tokens)\n",
        "\n",
        "  token_id = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "\n",
        "  for word, freq in counter.items():\n",
        "      token_id[word] = len(token_id)\n",
        "  token_id_size = len(token_id)\n",
        "  id_token = {idx: word for word, idx in token_id.items()}\n",
        "\n",
        "  return token_id, token_id_size, id_token\n",
        "\n",
        "def tokens_to_ids(tokens, vocab, unk_token=\"<unk>\"):\n",
        "    unk_idx = vocab[unk_token]\n",
        "    return [vocab.get(tok, unk_idx) for tok in tokens]\n",
        "\n",
        "def ids_to_tokens(ids, token_id, id_token, pad_token=\"<pad>\"):\n",
        "    pad_idx = token_id[pad_token]\n",
        "    return [id_token[idx] for idx in ids if idx != pad_idx]"
      ],
      "metadata": {
        "id": "SAVQDYU7I8Jh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GloVe_embedding(vocab):\n",
        "\n",
        "  # Glove embedding\n",
        "  embedding_dim = 200 #should check more\n",
        "  #glove_path = \"glove.twitter.27B.100d.txt\" #looks nice from kaggle, I will try the origin first\n",
        "\n",
        "  #extract original GloVe\n",
        "  glove_path = \"glove.6B.200d.txt\" #download the zip file and extract it manually cause it takes forever othewise\n",
        "  embedding_matrix = np.random.uniform(\n",
        "      -0.25, 0.25, (len(vocab), embedding_dim)\n",
        "  ).astype(\"float32\")\n",
        "\n",
        "  print(\"Loading GloVe embeddings...\")\n",
        "\n",
        "  with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
        "      for line in f:\n",
        "          values = line.strip().split()\n",
        "          word = values[0]\n",
        "          vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "\n",
        "          if word in vocab:\n",
        "              idx = vocab[word]\n",
        "              embedding_matrix[idx] = vector\n",
        "\n",
        "  print('Done')\n",
        "  return embedding_matrix, embedding_dim"
      ],
      "metadata": {
        "id": "ufHavl88JfDj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_token_to_id, train_vocab_size, train_id_to_token = token_id_mapping(train_tokens)\n",
        "test_token_to_id, test_vocab_size, test_id_to_token = token_id_mapping(test_tokens)\n",
        "Gl_train_embedding_matrix, Gl_train_embedding_dim = GloVe_embedding(train_token_to_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3sWKyYT3vY5",
        "outputId": "7e72a346-c080-4c36-d581-b8a82c1cd7ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings...\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "Rx6aWxLmjtzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_data_loader(Dataset):\n",
        "    def __init__(self, tweets, labels):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = torch.tensor(self.tweets[idx], dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return seq, label\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    seqs, labels = zip(*batch)\n",
        "    seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels)\n",
        "    return seqs, labels"
      ],
      "metadata": {
        "id": "-G_N9ty2kHJg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = [tokens_to_ids(t, train_token_to_id) for t in train_tokens]\n",
        "test_ids = [tokens_to_ids(t, test_token_to_id) for t in test_tokens]\n",
        "\n",
        "train_dataset = LSTM_data_loader(train_ids, train_labels)\n",
        "test_dataset = LSTM_data_loader(test_ids, test_labels)\n",
        "\n",
        "LSTM_train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "LSTM_test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "ubKaWwo4kg0s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-9zg-coOiIA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, token_id_size, embed_dim, hidden_dim, num_classes, embedding_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(token_id_size, embed_dim)\n",
        "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, (h, c) = self.lstm(x)\n",
        "        logits = self.fc(h[-1])\n",
        "        return logits\n",
        "\n",
        "\n",
        "def compute_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return f'Accuracy: {100 * correct / total}'\n"
      ],
      "metadata": {
        "id": "vEJLHsoCiM0K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "ZyZUdEUOilEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_LSTM(vocab_size, embed_dim, hidden_dim, embedding_matrix, num_epochs, path):\n",
        "  model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, 6, embedding_matrix)\n",
        "  loss_objective = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  # --- Training Loop ---\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loop = tqdm(LSTM_train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_x, batch_y in loop:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(batch_x)\n",
        "        loss = loss_objective(logits, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    acc = compute_accuracy(model, LSTM_test_loader)\n",
        "    print(f\"epoch {epoch+1} | loss {loss.item():.4f} | {acc}\")\n",
        "  torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "OH3D3uDDinoy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def double_check(mapping_func, token_to_id, id_to_token, vocab_size, embedding_matrix, path):\n",
        "\n",
        "  # Make sure the model is in evaluation mode\n",
        "  model =LSTMClassifier(vocab_size, 200, 256, 6, embedding_matrix)\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  model.eval()\n",
        "\n",
        "  # Pick 10 examples\n",
        "  num_examples = 20\n",
        "  examples_printed = 0\n",
        "  true, all = 0,0\n",
        "  with torch.no_grad():\n",
        "      for x, y in LSTM_test_loader:\n",
        "          logits = model(x)\n",
        "          preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "          for i in range(len(x)):\n",
        "              # print(\"Tweet:\", x[i])  # if x[i] is token ids, you may need to decode\n",
        "              # print(\"True label:\", y[i].item())\n",
        "              # print(\"Predicted label:\", preds[i].item())\n",
        "              # print(\"---\")\n",
        "              tweet_ids = x[i].tolist()\n",
        "              tweet_words = mapping_func(tweet_ids, token_to_id, id_to_token)\n",
        "              print(\" \".join(tweet_words))\n",
        "              if y[i].item() == preds[i].item():\n",
        "                  true += 1\n",
        "              all += 1\n",
        "              examples_printed += 1\n",
        "              if examples_printed >= num_examples:\n",
        "                  break\n",
        "          if examples_printed >= num_examples:\n",
        "              break\n",
        "  print('Accracy is:', 100*true/all)"
      ],
      "metadata": {
        "id": "GgV0BVjwrQ6_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'Gl_lstm_model.pth'\n",
        "embed_dim, hidden_dim, num_epochs = Gl_train_embedding_dim, 256, 10\n",
        "train_LSTM(train_vocab_size, embed_dim, hidden_dim, Gl_train_embedding_matrix, num_epochs, model_path)\n",
        "double_check(ids_to_tokens, test_token_to_id, test_id_to_token, train_vocab_size, Gl_train_embedding_matrix, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJZZGlpD1-48",
        "outputId": "5fb6428d-37b5-4a73-e034-83567cf7e246"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 500/500 [01:05<00:00,  7.68it/s, loss=1.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 | loss 1.4912 | Accuracy: 34.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 500/500 [01:04<00:00,  7.79it/s, loss=1.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 | loss 1.7754 | Accuracy: 35.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 500/500 [01:06<00:00,  7.55it/s, loss=1.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 | loss 1.7760 | Accuracy: 35.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 500/500 [01:04<00:00,  7.79it/s, loss=1.55]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 | loss 1.5500 | Accuracy: 35.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 500/500 [01:03<00:00,  7.85it/s, loss=1.63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 | loss 1.6264 | Accuracy: 34.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 500/500 [01:02<00:00,  7.98it/s, loss=1.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 | loss 1.4775 | Accuracy: 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 500/500 [01:02<00:00,  7.94it/s, loss=1.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 | loss 1.1110 | Accuracy: 58.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 500/500 [01:04<00:00,  7.79it/s, loss=0.848]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 | loss 0.8481 | Accuracy: 71.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 500/500 [01:07<00:00,  7.45it/s, loss=0.167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9 | loss 0.1667 | Accuracy: 86.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 500/500 [01:02<00:00,  7.98it/s, loss=0.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10 | loss 0.4496 | Accuracy: 89.2\n",
            "im feeling quite sad and sorry for myself but ill snap out of it soon\n",
            "i feel like i am still looking at a blank canvas blank pieces of paper\n",
            "i feel like a faithful <unk>\n",
            "i am just feeling cranky and blue\n",
            "i can have for a treat or if i am feeling festive\n",
            "i start to feel more appreciative of what god has done for me\n",
            "i am feeling more confident that we will be able to take care of this baby\n",
            "i feel incredibly lucky just to be able to talk to her\n",
            "i feel less keen about the army every day\n",
            "i feel dirty and ashamed for saying that\n",
            "i feel bitchy but not defeated yet\n",
            "i was dribbling on mums coffee table looking out of the window and feeling very happy\n",
            "i woke up often got up around am feeling <unk> radiation and groggy\n",
            "i was feeling sentimental\n",
            "i walked out of there an hour and fifteen minutes later feeling like i had been beaten with a stick and then placed on the rack and stretched\n",
            "i never stop feeling thankful as to compare with others i considered myself lucky because i did not encounter ruthless <unk> and i did not have to witness the <unk> of others\n",
            "i didn t feel abused and quite honestly it made my day a little better\n",
            "i know what it feels like he stressed glaring down at her as she <unk> more soap onto her <unk>\n",
            "i also loved that you could really feel the desperation in these <unk> and i especially liked the emotion between knight and <unk> as theyve been together in a similar fashion to batman and robin for a long time now\n",
            "i had lunch with an old friend and it was nice but in general im not feeling energetic\n",
            "Accracy is: 95.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To Do"
      ],
      "metadata": {
        "id": "XaBOKsoLCH4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement a GRU model.\n",
        "\n",
        "2. Experiment with hyperparameters and consider plotting a graph for epochs (to potentially find the \"optimal\" point)."
      ],
      "metadata": {
        "id": "x2N76jweCKuQ"
      }
    }
  ]
}